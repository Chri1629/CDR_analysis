\section{Performance Evaluation}
Per valutare i risultati sono stati utilizzati diversi indici, primo fra tutti l'\textit{accuracy}. L'accuracy misura l'affidabilità del modello a predire risultati corretti su nuovi dati.Per calcolare l'accuracy sul dataset $D$, con $n$ entrate, a predire la variabile $y_i$ dati i valori $\{x_i\}$ ($f(x_i)$ rappresenta la previsione) si sfrutta la seguente formula:
\[acc(D) = 1 - \frac{1}{n}\sum_{j=1}^{n}L(y_i,f(x_i))\]
L rappresenta la  \textit{loss function}, funzione che assegna valore 1 se $y_i \neq f(x_i)$ (cioè la predizione è errata) e 0 viceversa. Si può notare quindi che all'aumentare delle previsione errate diminuisce il valore dell'accuracy.
I modelli utilizzati restituiscono la matrice di confusione. Questa contiene al suo interno diversi valori, nel nostro caso i valori di CDR che potevano essere predetti erano 0, 0.5, 1 e 2. Un esempio di matrice di confusione appare così:
\[
  \begin{bmatrix}
    \frac{CDR}{Pred(CDR)} & 0 & 0.5 & 1 & 2 \\
    0 & 60 & 8 & 0 & 0 \\
    0.5 & 12 & 18 & 6 & 0 \\
    1 & 1 & 1 & 10 & 0 \\
    2 & 0 & 1 & 0 & 0 \\
  \end{bmatrix}
\]

Sulla diagonale si possono notare i valori previsti correttamente dal modello, mentre gli altri rappresentano valori previsti in maneira scorretta.
La accuracy è calcolata sommando i valori sulla diagonale e dividendoli per il totale. 

Il secondo indice utilizzato è la \textit{precision}. Viene definita come il rapporto tra i valori predetti correttamente della variabile che consideriamo diviso il numero di valori totali che appaiono nel dataset per quella variabile. Per esempio la precision della variabile 0.5 è:
\[P(0.5) = \frac{18}{12+18+6} = 0.50\]

Un altro indice sfruttato è la \textit{recall}. La recall viene calcolata sfruttando di nuovo il numero di valori predetti correttamente per la variabile considerata diviso il numero di valori totali predetti dal modello per quella variabile. Nuovamente riproponiamo come esempio la variabile 0.5, per cui la recall sarà:
\[R(0.5) = \frac{18}{8+18+1+1} = 0.64\]

L'ultimo indice che abbiamo sfruttato è la \textit{F-measure}. Viene calcolato sfruttando gli indici elencati precedentemente:
\[F = \frac{2\cdot r \cdot p}{r + p}\]
Valori elevati della F-measure indicano valori elevati di entrambi recall e precision. Pre concludere l'esempio portato precedentemente otteniamo:
\[F(0.5) = \frac{2 \cdot 0.50 \cdot 0.64}{0.50 + 0.64}=0.56\]


\subsection{Holdout}



La tecnica di Holdout consiste nel dividere il dataset in due partizioni in maniera casuale. Questo metodo permette di sfruttare una parte del dataset come \textit{train}, per addestrare il codice, e una parte come \textit{test}, per valutare le predizioni fatte. Le misure di accuracy vengono eseguite sulla partizione di test. 
La problematica principale di questa tecnica è la dipendenza dell'accuracy dal partizionamento iniziale. Determinate divisioni potrebbero sovrastimare o sottostimare l'accuracy.


\begin{table}[H]
\tabcolsep=0.10cm
\small
    \centering
    \begin{tabular}{ccccc}
     Modello & Recall & Precision & F-Measure & Accuracy  \\
     \hline
     MLP & 0.487 & 0.626 & 0.637 & 0.709 \\
     NB & 0.433 & 0.514 & (?) & 0.726 \\
     NBTree & 0.444 & 0.596 & 0.582 & 0.701 \\
     J48 & 0.415 & 0.567 & 0.558 & 0.709 \\
     RF & 0.458 & 0.531 & (?) & 0.735 \\
     Log & 0.436 & 0.604 & 0.588 & 0.701 \\
    \hline    
    \end{tabular}
    \caption{}
    \label{nuioahnaumdfuamhmadsuamhasfudhdsfmhmfasdmhm}
\end{table}




\subsection{Iterated Holdout}

La tecnica dell'iterated holdout consiste nell'iterare il metodo descritto nel paragrafo precedente. In tal modo si cerca di superare l'incoveniente della dipendenza dal partizionamento eseguendo più volte esso in maniera sempre casuale. La accuracy è calcolata semplicemente con una media di tutti i risultati ottenuti dai vari partizionamenti. La criticità del metodo risiede nell'impossibilità di controllare le divisioni del dataset, che in caso di valori molto sbilanciati potrebbe portare a risultati poco sensati di accuracy.


\begin{table}[H]
\tabcolsep=0.10cm
\small
\begin{tabular}{ccccc}

     Modello & Recall & Precision & F-Measure & Accuracy  \\
     \hline
     MLP & 0.491 & 0.619 & 0.660 & 0.702 \\
     NB & 0.475 & 0.674 & 0.650 & 0.727 \\
     NBTree & 0.450 & 0.680 & 0.608 & 0.717 \\
     J48 & 0.467 & 0.642 & 0.627 & 0.696 \\
     RF & 0.476 & 0.680 & 0.656 & 0.727 \\
     Log & 0.486 & 0.547 & 0.647 & 0.726 \\
     \hline
\end{tabular}
\end{table}


\subsection{Cross-validation}
La cross-validation utilizza un metodo leggermente diverso dall'iterated holdout. Il dataset viene suddiviso in diverse parti, di cui una verrà utilizzata come test e le altre come train. La accuracy viene calcolata nuovamente con una media.
La cross-validation permette inoltre di controllare il metodo di partizionamento, nel nostro caso abbiamo utilizzato un campionamento stratificato, a causa dei valori abbastanza sbilanciati della classe CDR. In tal modo abbiamo potuto garantire all'interno di ogni partizione la giusta proporzione tra i vari punteggi.

\begin{table}[H]
\tabcolsep=0.10cm
\small
\begin{tabular}{c c c c c}
     Modello & Recall & Precision & F-Measure & Accuracy  \\
     \hline
     MLP & 0.491 & 0.619 & 0.660 & 0.702 \\
     NB & 0.475 & 0.674 & 0.650 & 0.727 \\
     NBTree & 0.450 & 0.680 & 0.608 & 0.717 \\
     J48 & 0.467 & 0.642 & 0.627 & 0.696 \\
     RF & 0.476 & 0.680 & 0.656 & 0.727 \\
     Log & 0.486 & 0.547 & 0.647 & 0.726 \\
     \hline
\end{tabular}
\end{table}
\subsection{Feature selection}